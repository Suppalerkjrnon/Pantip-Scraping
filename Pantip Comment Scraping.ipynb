{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from requests.exceptions import RequestException, HTTPError, ConnectionError, Timeout\n",
    "from json import JSONDecodeError\n",
    "from pandas import json_normalize\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "#Iterate all of the dictionary\n",
    "path = 'replace with your path that contains the json/pickle files that have a list of topic id and key(keyword)'\n",
    "\n",
    "response_dict_list = []\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith('.pkl'):\n",
    "        with open(f'{path}/{filename}', 'rb') as f:\n",
    "            response_dict = pickle.load(f)\n",
    "            response_dict_list.append(response_dict)\n",
    "            \n",
    "print(len(response_dict_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty dictionary to store results\n",
    "extracted_ids_keyword_dict = {}\n",
    "\n",
    "def update_extracted_ids(response_dict_list, extracted_ids_keyword_dict):\n",
    "    for response_dict in response_dict_list:\n",
    "        for key, value in response_dict.items():\n",
    "            if isinstance(value,dict) and 'search_keyword' in value.keys():\n",
    "                search_keyword = value['search_keyword']  # Ensure 'search_keyword' exists\n",
    "                if search_keyword:\n",
    "                    if search_keyword not in extracted_ids_keyword_dict:\n",
    "                        extracted_ids_keyword_dict[search_keyword] = []\n",
    "\n",
    "                    for k, v in value.items():\n",
    "                        if isinstance(v, dict) and 'data' in v.keys():\n",
    "                            for data_item in v['data']:\n",
    "                                extracted_ids_keyword_dict[search_keyword].append(data_item['id'])\n",
    "\n",
    "    return extracted_ids_keyword_dict\n",
    "\n",
    "# Update the dictionary with data from response_dict_list\n",
    "extracted_ids_keyword_dict = update_extracted_ids(response_dict_list, extracted_ids_keyword_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_ids_keyword_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['บำนาญ', 'หุ้นปันผล', 'หุ้นต่างประเทศ', 'อสังหา', 'หุ้นเน้นปันผล ', 'กองทุน', 'สินทรัพย์', 'วางแผนชีวิต', 'วางแผนครอบครัว', 'แผนดูแลพ่อแม่', 'Work-Life Balance', 'เก็บเงินแต่งงาน', 'ให้เงินภรรยา', 'ส่งเงินให้ที่บ้าน', 'ดูแลพ่อแม่', 'เสียสละเพื่อครอบครัว', 'ทดแทนบุญคุณ', 'พ่อแม่ป่วย', 'ประกัน', 'ข้อมูลสุขภาพประกัน', 'เดินตามรอย', 'heart rate', 'ข้อมูลสุขภาพ', 'Passive Income', 'กระแสเงินสด', 'เป้าหมายการเงิน', 'รายได้เสริม', 'อาชีพเสริม', 'รายได้2ทาง', 'อิสรภาพทางการเงิน', 'รายได้หลายทาง', 'แยกกระเป๋า', 'กลยุทธ์การลงทุน', 'แนวทางการลงทุน', 'กลยุทธ์การเงิน', 'Early retire', 'เกษียณก่อนวัย', 'เก็บเงินไว้ใช้ตอนเกษียณ', 'เงินเก็บฉุกเฉิน', 'การวางแผนการเงิน', 'เงินฉุกเฉิน', 'ลดค่าใช้จ่าย', 'ลดหย่อนภาษี', 'วินัยทางการเงิน', 'คำแนะนำการลงทุน', 'หนี้สิน', 'หุ้นกู้'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_ids_keyword_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "บำนาญ: 10000\n",
      "หุ้นปันผล: 10000\n",
      "หุ้นต่างประเทศ: 7240\n",
      "อสังหา: 10000\n",
      "หุ้นเน้นปันผล : 1792\n",
      "กองทุน: 10000\n",
      "สินทรัพย์: 10000\n",
      "วางแผนชีวิต: 10000\n",
      "วางแผนครอบครัว: 1940\n",
      "แผนดูแลพ่อแม่: 0\n",
      "Work-Life Balance: 620\n",
      "เก็บเงินแต่งงาน: 4262\n",
      "ให้เงินภรรยา: 886\n",
      "ส่งเงินให้ที่บ้าน: 2950\n",
      "ดูแลพ่อแม่: 10000\n",
      "เสียสละเพื่อครอบครัว: 606\n",
      "ทดแทนบุญคุณ: 4428\n",
      "พ่อแม่ป่วย: 4488\n",
      "ประกัน: 10000\n",
      "ข้อมูลสุขภาพประกัน: 575\n",
      "เดินตามรอย: 5707\n",
      "heart rate: 3138\n",
      "ข้อมูลสุขภาพ: 4607\n",
      "Passive Income: 5058\n",
      "กระแสเงินสด: 9230\n",
      "เป้าหมายการเงิน: 1014\n",
      "รายได้เสริม: 10000\n",
      "อาชีพเสริม: 10000\n",
      "รายได้2ทาง: 909\n",
      "อิสรภาพทางการเงิน: 1355\n",
      "รายได้หลายทาง: 2276\n",
      "แยกกระเป๋า: 4970\n",
      "กลยุทธ์การลงทุน: 5682\n",
      "แนวทางการลงทุน: 3004\n",
      "กลยุทธ์การเงิน: 375\n",
      "Early retire: 703\n",
      "เกษียณก่อนวัย: 284\n",
      "เก็บเงินไว้ใช้ตอนเกษียณ: 109\n",
      "เงินเก็บฉุกเฉิน: 1383\n",
      "การวางแผนการเงิน: 3640\n",
      "เงินฉุกเฉิน: 7199\n",
      "ลดค่าใช้จ่าย: 10000\n",
      "ลดหย่อนภาษี: 10000\n",
      "วินัยทางการเงิน: 3349\n",
      "คำแนะนำการลงทุน: 2080\n",
      "หนี้สิน: 10000\n",
      "หุ้นกู้: 10000\n"
     ]
    }
   ],
   "source": [
    "for key, value in extracted_ids_keyword_dict.items():\n",
    "    print(f'{key}: {len(value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Scraping the Comment by sending topic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_comments(key, comment_ids):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\",\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\"\n",
    "    }\n",
    "\n",
    "    all_comments = {}\n",
    "\n",
    "    for comment_id in comment_ids:\n",
    "        params = {\n",
    "            'tid': comment_id,\n",
    "            \"type\": \"3\"\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            r = requests.get(\"http://pantip.com/forum/topic/render_comments\", params=params, headers=headers)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            if r.status_code == 200:\n",
    "                try:\n",
    "                    comment_data = r.json()\n",
    "                    all_comments[comment_id] = comment_data\n",
    "                except json.JSONDecodeError as json_error:\n",
    "                    print(f\"JSON decoding error for {comment_id}: {json_error}\")\n",
    "            else:\n",
    "                print(f\"Unexpected status code {r.status_code} for {comment_id}\")\n",
    "\n",
    "        except (RequestException, HTTPError, ConnectionError, Timeout) as e:\n",
    "            print(f\"Request error for {comment_id}: {e}\")\n",
    "    \n",
    "    return key, all_comments\n",
    "\n",
    "def get_comment_ids(extracted_ids_keyword_dict, max_workers=10):\n",
    "    comment_dict = {}\n",
    "    # Limit the number of concurrent requests, this will be assigned to each thread \n",
    "    batch_size = 600\n",
    "    \n",
    "    # Use ThreadPoolExecutor to fetch comments concurrently\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for key, value in extracted_ids_keyword_dict.items():\n",
    "            if isinstance(value, list):\n",
    "                # Create batches of up to 600 IDs, this will be divided among threads (20 threads = 12,000 IDs)\n",
    "                batches = [value[i:i+batch_size] for i in range(0, len(value), batch_size)]\n",
    "                # Submit each batch to the executor\n",
    "                for batch in batches:\n",
    "                    futures.append(executor.submit(fetch_comments, key, batch))\n",
    "        \n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            key, result = future.result()\n",
    "            if result:\n",
    "                if key not in comment_dict:\n",
    "                    comment_dict[key] = []\n",
    "                comment_dict[key].append(result)\n",
    "            \n",
    "            time.sleep(0.5)  # Add a small delay between requests\n",
    "    \n",
    "    # Merge results for each key into a single dictionary, for final output\n",
    "    for key in comment_dict:\n",
    "        merged_result = {}\n",
    "        # Merge all results for each key from every batch\n",
    "        for batch_result in comment_dict[key]:\n",
    "            merged_result.update(batch_result)\n",
    "        comment_dict[key] = merged_result\n",
    "    \n",
    "    return comment_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seprarate the dictionary into 3, where key is divided into 3\n",
    "def divide_dict(dictionary):\n",
    "    dict1 = {}\n",
    "    dict2 = {}\n",
    "    dict3 = {}\n",
    "    \n",
    "    for i, key in enumerate(dictionary.keys()):\n",
    "        if i % 3 == 0:\n",
    "            dict1[key] = dictionary[key]\n",
    "        elif i % 3 == 1:\n",
    "            dict2[key] = dictionary[key]\n",
    "        else:\n",
    "            dict3[key] = dictionary[key]\n",
    "    \n",
    "    return dict1, dict2, dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 3 separate dictionaries for each thread\n",
    "filtered_dict1, filtered_dict2, filtered_dict3 = divide_dict(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Investigate a keys to prevent duplication\n",
    "filtered_dict1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dict3.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = get_comment_ids(filtered_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = get_comment_ids(filtered_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result3 = get_comment_ids(filtered_dict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a DataFrame from the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.DataFrame.from_dict(result3, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limit column to show only show 5 columns\n",
    "pd.set_option('display.max_columns', 10)\n",
    "\n",
    "comment_df.head(15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
